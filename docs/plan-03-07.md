# Plan for UI/UX Improvements (03-07)

This document outlines a plan to enhance the user interface (UI) and user experience (UX) of the WebLLM chat application. The focus is on providing better feedback to the user during loading and generation, and improving the overall usability of the chat interface.

The tasks are broken down into small, committable, and manually testable steps.

---

## Phase 1: Improve Loading and Initialization Feedback

The goal of this phase is to provide clear feedback to the user while the AI model is being downloaded and initialized.

### Tasks

1.  **[x] Display Model Download Progress:**

    - **Goal:** Show a progress bar or percentage to the user, so they know the model is being downloaded and it's not stuck.
    - **Implementation:**
        - In `App.tsx`, handle the `download-progress` message from the `WebLLM` worker.
        - Create a new state variable, e.g., `downloadProgress`, to store the progress information.
        - Render a progress indicator in the `ChatDrawer` component when `downloadProgress` is available.
    - **Testing:** Open the app with a cleared cache to trigger the model download and verify the progress indicator is shown.

2.  **[x] Indicate Model Initialization:**

    - **Goal:** Inform the user that the model is being initialized after download.
    - **Implementation:**
        - Introduce a new state, e.g., `isInitializing`, in `App.tsx`.
        - Set `isInitializing` to `true` when the first prompt is sent and the model is not ready yet.
        - Display a loading spinner or a message like "Initializing model..." in the `ChatDrawer`.
        - The worker sends a `ready` message. We should modify this to be more explicit about when the pipeline is _actually_ ready to be used.
    - **Testing:** The first time a user sends a message, a loading indicator should appear until the model is ready to generate tokens.

3.  **[x] Disable Chat Input During Loading:**
    - **Goal:** Prevent the user from sending messages while the model is still loading or initializing.
    - **Implementation:**
        - Pass a `disabled` prop to the `ChatInput` component based on the application's loading/initializing state.
    - **Testing:** The chat input should be disabled until the model is fully ready to accept prompts.

---

## Phase 2: Enhance Chat Interaction Experience

This phase focuses on improving the core chat experience.

### Tasks

1.  **Disable Input During Generation:**

    - **Goal:** Prevent users from sending a new message while the assistant is already generating a response.
    - **Implementation:**
        - Create a new state, e.g., `isGenerating`, in `App.tsx`.
        - Set `isGenerating` to `true` when a prompt is sent and set it to `false` when the `generation-complete` message is received.
        - Pass a `disabled` prop to the `ChatInput` based on this state.
    - **Testing:** Send a message and verify that the input field is disabled until the response is complete.

2.  **Handle and Display LLM 'Thinking' Process:**

    - **Goal:** Separate the LLM's internal "thinking" process from the final response and provide a way for users to view it.
    - **Implementation:**
        - **Data Structure:** Update the `ChatMessage` type in `src/components/chat/ChatMessage.type.ts` to include an optional `thinking: string` field.
        - **Parsing Logic:** In `App.tsx`, modify the `token` message handler to detect and parse `<think>...</think>` blocks from the streamed response. Content inside the tags should be saved to the `thinking` field of the assistant's message, while the rest is saved to the `content` field.
        - **UI Component:** In `ChatMessage.tsx`, for messages with `thinking` content, add a toggle (e.g., a button or an icon) to show/hide the thinking process.
        - The thinking content should be displayed in a distinct, formatted block (e.g., a collapsible section with a different background color).
    - **Testing:**
        - Craft a prompt that will cause the LLM to output `<think>` blocks.
        - Verify the main response is clean (no tags).
        - Verify the "Show Thinking" toggle appears and works correctly, showing the thinking content on click.
        - Ensure the UI handles streamed responses with partial tags gracefully.

3.  **Improve Error Display:**

    - **Goal:** Show errors in a more user-friendly way than just a JSON string.
    - **Implementation:**
        - In `App.tsx`, when an `error` message is received, display a formatted and easy-to-understand error message in the chat. For example: "Sorry, an error occurred. Please try again."
    - **Testing:** Manually simulate an error from the worker to check the display.

4.  **Auto-resizing Textarea for Chat Input:**
    - **Goal:** Replace the single-line `<input>` with a `<textarea>` that grows with the user's text.
    - **Implementation:**
        - In `ChatInput.tsx`, replace the `<input>` element with a `<textarea>`.
        - Use a lightweight library or a simple JavaScript solution to make the textarea auto-sizing.
    - **Testing:** Type multiple lines of text into the chat input and verify it resizes correctly.

---

## Phase 3: UI Polish and Refinements

This phase is for small touches that make the application feel more polished.

### Tasks

1.  **Add a "Copy to Clipboard" Button for Messages:**

    - **Goal:** Allow users to easily copy the content of a chat message.
    - **Implementation:**
        - In `ChatMessage.tsx`, add a copy icon/button.
        - On click, use the `navigator.clipboard.writeText()` API to copy the message content.
        - Show feedback to the user, e.g., by changing the icon or showing a tooltip "Copied!".
    - **Testing:** Click the copy button and paste the content somewhere else to verify.

2.  **Replace Placeholder Content:**
    - **Goal:** Replace the "Hello World" placeholder on the main page with something more meaningful.
    - **Implementation:**
        - Update the `h1` in `App.tsx` to something like "WebLLM Chat" or provide some instructions.
    - **Testing:** The main page should show the new content.

This plan provides a clear path forward. Each step is small and can be implemented and tested independently, allowing for an iterative development process.
