# Plan for UI/UX Improvements (03-07)

This document outlines a plan to enhance the user interface (UI) and user experience (UX) of the WebLLM chat application. The focus is on providing better feedback to the user during loading and generation, and improving the overall usability of the chat interface.

The tasks are broken down into small, committable, and manually testable steps.

---

## Phase 1: Improve Loading and Initialization Feedback

The goal of this phase is to provide clear feedback to the user while the AI model is being downloaded and initialized.

### Tasks

1.  **[x] Display Model Download Progress:**

    - **Goal:** Show a progress bar or percentage to the user, so they know the model is being downloaded and it's not stuck.
    - **Implementation:**
        - In `App.tsx`, handle the `download-progress` message from the `WebLLM` worker.
        - Create a new state variable, e.g., `downloadProgress`, to store the progress information.
        - Render a progress indicator in the `ChatDrawer` component when `downloadProgress` is available.
    - **Testing:** Open the app with a cleared cache to trigger the model download and verify the progress indicator is shown.

2.  **[x] Indicate Model Initialization:**

    - **Goal:** Inform the user that the model is being initialized after download.
    - **Implementation:**
        - Introduce a new state, e.g., `isInitializing`, in `App.tsx`.
        - Set `isInitializing` to `true` when the first prompt is sent and the model is not ready yet.
        - Display a loading spinner or a message like "Initializing model..." in the `ChatDrawer`.
        - The worker sends a `ready` message. We should modify this to be more explicit about when the pipeline is _actually_ ready to be used.
    - **Testing:** The first time a user sends a message, a loading indicator should appear until the model is ready to generate tokens.

3.  **[x] Disable Chat Input During Loading:**
    - **Goal:** Prevent the user from sending messages while the model is still loading or initializing.
    - **Implementation:**
        - Pass a `disabled` prop to the `ChatInput` component based on the application's loading/initializing state.
    - **Testing:** The chat input should be disabled until the model is fully ready to accept prompts.

---

## Phase 2: Enhance Chat Interaction Experience

This phase focuses on improving the core chat experience.

### Tasks

1.  **[x] Disable Input During Generation:**

    - **Goal:** Prevent users from sending a new message while the assistant is already generating a response.
    - **Implementation:**
        - Create a new state, e.g., `isGenerating`, in `App.tsx`.
        - Set `isGenerating` to `true` when a prompt is sent and set it to `false` when the `generation-complete` message is received.
        - Pass a `disabled` prop to the `ChatInput` based on this state.
    - **Testing:** Send a message and verify that the input field is disabled until the response is complete.

2.  **[x] Handle and Display LLM 'Thinking' Process:**

    - **Goal:** Separate the LLM's internal "thinking" process from the final response and provide a way for users to view it.
    - **Implementation:**
        - **Data Structure:** Update the `ChatMessage` type in `src/components/chat/ChatMessage.type.ts` to include an optional `thinking: string` field.
        - **Parsing Logic:** In `App.tsx`, modify the `token` message handler to detect and parse `<think>...</think>` blocks from the streamed response. Content inside the tags should be saved to the `thinking` field of the assistant's message, while the rest is saved to the `content` field.
        - **UI Component:** In `ChatMessage.tsx`, for messages with `thinking` content, add a toggle (e.g., a button or an icon) to show/hide the thinking process.
        - The thinking content should be displayed in a distinct, formatted block (e.g., a collapsible section with a different background color).
    - **Testing:**
        - Craft a prompt that will cause the LLM to output `<think>` blocks.
        - Verify the main response is clean (no tags).
        - Verify the "Show Thinking" toggle appears and works correctly, showing the thinking content on click.
        - Ensure the UI handles streamed responses with partial tags gracefully.

3.  **[x] Improve Error Display:**

    - **Goal:** Show errors in a more user-friendly way than just a JSON string.
    - **Implementation:**
        - In `App.tsx`, when an `error` message is received, display a formatted and easy-to-understand error message in the chat. For example: "Sorry, an error occurred. Please try again."
    - **Testing:** Manually simulate an error from the worker to check the display.

4.  **[x] Auto-resizing Textarea for Chat Input:**
    - **Goal:** Replace the single-line `<input>` with a `<textarea>` that grows with the user's text.
    - **Implementation:**
        - In `ChatInput.tsx`, replace the `<input>` element with a `<textarea>`.
        - Use a lightweight library or a simple JavaScript solution to make the textarea auto-sizing.
    - **Testing:** Type multiple lines of text into the chat input and verify it resizes correctly.

---

## Phase 3: UI Polish and Refinements

This phase is for small touches that make the application feel more polished.

### Tasks

1.  **[x] Add a "Copy to Clipboard" Button for Messages:**

    - **Goal:** Allow users to easily copy the content of a chat message.
    - **Implementation:**
        - In `ChatMessage.tsx`, add a copy icon/button.
        - On click, use the `navigator.clipboard.writeText()` API to copy the message content.
        - Show feedback to the user, e.g., by changing the icon or showing a tooltip "Copied!".
    - **Testing:** Click the copy button and paste the content somewhere else to verify.

2.  **[x] Replace Placeholder Content:**
    - **Goal:** Replace the "Hello World" placeholder on the main page with something more meaningful.
    - **Implementation:**
        - Update the `h1` in `App.tsx` to something like "WebLLM Chat" or provide some instructions.
    - **Testing:** The main page should show the new content.

This plan provides a clear path forward. Each step is small and can be implemented and tested independently, allowing for an iterative development process.

---

## Technical Debt and Future Improvements

This section lists potential areas of improvement and refactoring that were identified during development but were not addressed as part of the initial implementation.

1.  **Improve streaming parser:** The current parser for `<think>` tags is based on string splitting and may not be robust enough for edge cases like nested or malformed tags. It should be replaced with a more resilient parsing mechanism.

2.  **Optimize streaming parser:** The current implementation re-parses the entire message on each new token. For long responses, this can be inefficient. This should be refactored to a more performant stateful parser that does not need to re-process the entire stream.

3.  **Refactor message stream handling:** The logic for handling the raw message stream and parsing it is currently located in the main `App.tsx` component. This could be encapsulated into a dedicated custom hook (e.g., `useStreamingChat`) to simplify `App.tsx` and better separate business logic from the view layer.

---

## Phase 4: Conversation History & Context Chaining

This phase addresses a critical limitation: the current implementation treats each user message as a completely new conversation, ignoring all previous context. The AI model receives only the current prompt without any conversation history, which severely limits its ability to maintain coherent, contextual conversations.

### Current Problem Analysis

- **No Context Preservation:** Each request starts fresh with only a system message and the current user input
- **Missing Conversation Flow:** The AI cannot reference previous messages, follow up on topics, or maintain conversation continuity
- **Inefficient Context Usage:** The parsed "thinking" content is displayed to users but never used to provide context to the model
- **Limited Conversational AI Experience:** Users cannot have meaningful multi-turn conversations

### Tasks

1. **Add Context Window Visibility UI:**

    - **Goal:** Provide users with real-time visibility into their context window usage, showing how much of the available context is being used.
    - **Implementation:**
        - **Token Calculation:** Implement a token counting utility that estimates the total tokens in the current conversation context
        - **Context Meter:** Add a visual indicator in the chat interface showing current context usage (e.g., "Context: 2.3k / 8k tokens")
        - **Usage Categories:** Display token counts in user-friendly formats (0.1k, 1k, 10k, etc.)
        - **Real-time Updates:** Update the context meter after each interaction (both user messages and assistant responses)
        - **Visual Design:** Use a progress bar or meter that changes color as context approaches limits (green → yellow → red)
        - **Context Breakdown:** Optionally show breakdown of tokens by message type (system, user, assistant)
    - **Testing:**
        - Send messages and verify the context meter updates accurately
        - Test with various message lengths to ensure proper token estimation
        - Verify the UI updates immediately after each interaction
        - Test visual indicators change appropriately as context fills up

2. **Implement Conversation History Management:**

    - **Goal:** Maintain and send the full conversation history to the model for each new request, enabling contextual responses.
    - **Implementation:**
        - **Data Structure Changes:** Update the worker message interface to accept a full conversation history instead of just a single prompt string.
        - **Message Preparation:** In `App.tsx`, modify `handleSend` to prepare the complete conversation history from the `messages` state.
        - **Context Filtering:** Create a function to convert UI messages to model-compatible format, excluding internal fields like `id`, `_rawContent`, and `thinking` content.
        - **Worker Updates:** Update `transformers.worker.ts` to accept and process the full message history array.
    - **Testing:**
        - Send a message, then send a follow-up that references the previous message
        - Verify the AI can maintain context across multiple turns
        - Test with conversations of varying lengths

3. **Optimize Context Window Management:**

    - **Goal:** Prevent context window overflow while maintaining relevant conversation history.
    - **Implementation:**
        - **Token Estimation:** Implement a rough token counting mechanism to estimate the total context length
        - **History Truncation:** When approaching context limits, remove older messages while preserving the system message and recent context
        - **Smart Truncation Strategy:** Prioritize keeping recent messages and important context over older, less relevant exchanges
        - **Configurable Limits:** Set reasonable defaults for maximum context length (e.g., 75% of model's context window)
    - **Testing:**
        - Simulate very long conversations to trigger truncation
        - Verify that truncation maintains conversation coherence
        - Test with different conversation patterns (short vs. long messages)

4. **Exclude Thinking Content from Context:**

    - **Goal:** Prevent the model's internal "thinking" process from being fed back as context, which could confuse the model and waste tokens.
    - **Implementation:**
        - **Context Sanitization:** Ensure that when building the conversation history for the model, only the `content` field is used from assistant messages
        - **Thinking Isolation:** The `thinking` field should remain purely for user interface purposes and never be included in model context
        - **Clean Message Format:** Create a helper function to convert UI messages to clean model messages, stripping all UI-specific fields
    - **Testing:**
        - Verify that assistant messages in the context contain only the final response, not the thinking process
        - Test with messages that have extensive thinking content to ensure it's properly excluded
        - Confirm that the model doesn't reference its own thinking process in responses

5. **Enhance Message Interface and Type Safety:**

    - **Goal:** Update the communication interface between the frontend and worker to support full conversation history.
    - **Implementation:**
        - **Type Definitions:** Create proper TypeScript interfaces for model messages vs. UI messages
        - **Worker Message Format:** Update the worker to expect `{ messages: ModelMessage[] }` instead of `{ prompt: string }`
        - **WebLLM Service Updates:** Modify the `WebLLM` class to have a `generateWithHistory` method that accepts the full conversation
        - **Backward Compatibility:** Ensure the changes don't break existing functionality during the transition
    - **Testing:**
        - Verify type safety across the entire message flow
        - Test that the worker correctly processes the new message format
        - Ensure no runtime errors from interface mismatches

6. **Add Conversation Management Features:**

    - **Goal:** Provide users with control over their conversation context.
    - **Implementation:**
        - **Clear History:** Add a button/option to clear the conversation history and start fresh
        - **Context Indicator:** Show users when context is being truncated due to length limits
        - **System Message Customization:** Allow users to modify the system message for different conversation modes
        - **Export/Import:** Enable users to save and load conversation histories
    - **Testing:**
        - Test the clear history functionality resets the conversation properly
        - Verify context indicators appear at appropriate times
        - Test conversation export/import maintains message integrity

### Technical Considerations

- **Performance Impact:** Sending full conversation history increases payload size and processing time
- **Memory Usage:** Longer conversations will consume more memory in both the frontend and worker
- **Context Window Limits:** The Qwen3-0.6B model has specific context length limitations that must be respected
- **Streaming Compatibility:** Ensure that conversation history doesn't interfere with the streaming response mechanism

### Success Criteria

- Users can have coherent multi-turn conversations where the AI remembers previous exchanges
- The system handles long conversations gracefully without breaking or losing important context
- Context window management prevents errors while maintaining conversation quality
- The thinking content is properly isolated from the model's context
- The interface remains responsive and user-friendly despite the increased complexity

This phase transforms the application from a simple single-turn question-answering system into a true conversational AI interface, dramatically improving the user experience and the AI's ability to provide helpful, contextual responses.
